{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AutoML - distributed training for classification\n",
        "\n",
        "## Contents\n",
        "1. [Introduction](#Introduction)\n",
        "1. [Setup](#Setup)\n",
        "1. [Train](#Train)\n",
        "1. [Featurization transparency and model explanation](#Results)\n",
        "1. [Deploy](#Results)\n",
        "1. [Test](#Test)\n",
        "1. [Acknowledgements](#Acknowledgements)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In this example we use the associated credit card dataset to showcase how you can use AutoML distributed training for a simple classification problem. The goal is to predict if a credit card transaction is considered a fraudulent charge.\n",
        "\n",
        "This notebook is using multiple remote compute nodes to train the model. \n",
        "\n",
        "In this notebook you will learn how to:\n",
        "1. Create an experiment using an existing workspace.\n",
        "2. Configure AutoML using `AutoMLConfig` for distrbuted training\n",
        "3. Train the model using multiple remote compute nodes."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "As part of the setup you have already created an Azure ML `Workspace` object. For Automated ML you will need to create an `Experiment` object, which is a named object in a `Workspace` used to run experiments."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import logging\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import azureml.core\n",
        "from azureml.core.experiment import Experiment\n",
        "from azureml.core.workspace import Workspace\n",
        "from azureml.core.dataset import Dataset\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "from azureml.interpret import ExplanationClient"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675236224694
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This sample notebook may use features that are not available in previous versions of the Azure ML SDK."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.from_config()\n",
        "\n",
        "# choose a name for experiment\n",
        "experiment_name = \"automl-distributed-training-classification\"\n",
        "\n",
        "experiment = Experiment(ws, experiment_name)\n",
        "\n",
        "output = {}\n",
        "output[\"Subscription ID\"] = ws.subscription_id\n",
        "output[\"Workspace\"] = ws.name\n",
        "output[\"Resource Group\"] = ws.resource_group\n",
        "output[\"Location\"] = ws.location\n",
        "output[\"Experiment Name\"] = experiment.name\n",
        "output[\"SDK Version\"] = azureml.core.VERSION\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "outputDf = pd.DataFrame(data=output, index=[\"\"])\n",
        "outputDf.T"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675236227581
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create or Attach existing AmlCompute\n",
        "A compute target is required to execute the Automated ML run. In this tutorial, you create AmlCompute as your training compute resource.\n",
        "\n",
        "> Note that if you have an AzureML Data Scientist role, you will not have permission to create compute resources. Talk to your workspace or IT admin to create the compute targets described in this section, if they do not already exist.\n",
        "\n",
        "#### Creation of AmlCompute takes approximately 5 minutes. \n",
        "If the AmlCompute with that name is already in your workspace this code will skip the creation process.\n",
        "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# Choose a name for your CPU cluster\n",
        "cpu_cluster_name = \"automl-distributed-training\"\n",
        "\n",
        "# Verify that cluster does not exist already\n",
        "try:\n",
        "    compute_target = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
        "    print(\"Found existing cluster, use it.\")\n",
        "except ComputeTargetException:\n",
        "    compute_config = AmlCompute.provisioning_configuration(\n",
        "        vm_size=\"STANDARD_DS12_V2\", max_nodes=8\n",
        "    )\n",
        "    compute_target = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
        "compute_target.wait_for_completion(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675236228284
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data\n",
        "\n",
        "The training data this notebook using not large and is dynamically constructing the dataset.\n",
        "For large data we recommend that the dataset be registered in your workspace prior to running this notebook and use Dataset.get_by_name() API to retrieve training data as shown in the commented code below.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.data import DataType\n",
        "data = \"https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/creditcard.csv\"\n",
        "training_data = Dataset.Tabular.from_delimited_files(data, set_column_types={'Time': DataType.to_float()})\n",
        "\n",
        "# optional - validation data. Specify null for auto splitting or specify validation_size to control auto splitting size\n",
        "data_validate = \"https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/creditcard_validate.csv\"\n",
        "validation_data = Dataset.Tabular.from_delimited_files(data_validate, set_column_types={'Time': DataType.to_float()})\n",
        "\n",
        "# optional - validation data. Specify null for auto splitting or specify test_size to control auto splitting size\n",
        "data_test = \"https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/creditcard_test.csv\"\n",
        "test_data = Dataset.Tabular.from_delimited_files(data_test, set_column_types={'Time': DataType.to_float()})\n",
        "\n",
        "label_column_name = \"Class\"\n",
        "\n",
        "# Please register your large dataset in the worksapce with proper types configured for each column\n",
        "# And then please replace the above lines with following code \n",
        "# training_data = Dataset.get_by_name(ws, \"name-of-training-dataset\")\n",
        "# validation_data = Dataset.get_by_name(ws, \"name-of-validation-dataset\")\n",
        "# test_data = Dataset.get_by_name(ws, \"name-of-test-dataset\")\n",
        "# label_column_name = \"name-of-label-column\"\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "load-data",
        "gather": {
          "logged": 1675236233550
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train\n",
        "\n",
        "Instantiate a AutoMLConfig object. Besides the properties shown in the sample, specify the following properties to properly configure distributed training \n",
        "\n",
        "|Property|Description|\n",
        "|-|-|\n",
        "|**use_distributed**|specify true to enable distributed training. Default is false.|\n",
        "|**max_nodes**|Specify how many nodes you want to use for this job. Make sure the compute cluster you specify has these number of nodes allocated.|\n",
        "|**allowed_models**| Specify LightGBM as that is the only agorithm supported. Specifying this will not be necessary in the upcoming versions. |\n",
        "|**experiment_timeout_hours**| Specify few hours at least. Large data training takes more time. |\n",
        "|**validation_data**| Consider providing validation data. Specify null for auto splitting or specify test_size to control auto splitting size|\n",
        "|**test_data**| Consider providing test data. Specify null for auto splitting or specify test_size to control auto splitting size|\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "automl_settings = {\n",
        "    \"use_distributed\":True,\n",
        "    \"max_nodes\":8,\n",
        "    \"allowed_models\": [\"LightGBM\"],\n",
        "    \"experiment_timeout_hours\": 24,  \n",
        "    \"primary_metric\": \"average_precision_score_weighted\",\n",
        "    \"verbosity\": logging.INFO,\n",
        "}\n",
        "\n",
        "automl_config = AutoMLConfig(\n",
        "    task=\"classification\",\n",
        "    debug_log=\"automl_errors.log\",\n",
        "    compute_target=compute_target,\n",
        "    training_data=training_data,\n",
        "    validation_data=validation_data,\n",
        "    test_data=test_data, \n",
        "    label_column_name=label_column_name,\n",
        "    iterations=10,\n",
        "    **automl_settings,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "automl-config",
        "gather": {
          "logged": 1675236233764
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call the `submit` method on the experiment object and pass the run configuration. Depending on the data and the number of iterations this can run for a while. Validation errors and current status will be shown when setting `show_output=True` and the execution will be synchronous."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "remote_run = experiment.submit(automl_config, show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675237128379
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the best Run object\r\n",
        "best_run = remote_run.get_best_child()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675237129562
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Featurization transparency and model explanation\r\n",
        "\r\n",
        "View featurization summary for the best model - to study how different features were transformed. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the featurization summary JSON file locally\r\n",
        "best_run.download_file(\r\n",
        "    \"outputs/featurization_summary.json\", \"featurization_summary.json\"\r\n",
        ")\r\n",
        "\r\n",
        "# Render the JSON as a pandas DataFrame\r\n",
        "with open(\"featurization_summary.json\", \"r\") as f:\r\n",
        "    records = json.load(f)\r\n",
        "\r\n",
        "pd.DataFrame.from_records(records)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675237129906
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve the explanation from the best_run which includes explanations for engineered features and raw features. Make sure that the run for generating explanations for the best model is completed."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wait for the best model explanation run to complete\r\n",
        "from azureml.core.run import Run\r\n",
        "\r\n",
        "model_explainability_run_id = remote_run.id + \"_\" + \"ModelExplain\"\r\n",
        "print(model_explainability_run_id)\r\n",
        "model_explainability_run = Run(\r\n",
        "    experiment=experiment, run_id=model_explainability_run_id\r\n",
        ")\r\n",
        "model_explainability_run.wait_for_completion()\r\n",
        "\r\n",
        "client = ExplanationClient.from_run(best_run)\r\n",
        "engineered_explanations = client.download_model_explanation(raw=True) # specify false for raw feature names\r\n",
        "exp_data = engineered_explanations.get_feature_importance_dict()\r\n",
        "exp_data"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675237217450
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy\r\n",
        "\r\n",
        "### Retrieve the best model for deployment and register it\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = best_run.properties[\"model_name\"]\r\n",
        "script_file_name = \"inference/score.py\"\r\n",
        "best_run.download_file(\"outputs/scoring_file_v_1_0_0.py\", \"inference/score.py\")\r\n",
        "\r\n",
        "description = \"AutoML Model trained using distributed training\"\r\n",
        "tags = None\r\n",
        "model = remote_run.register_model(\r\n",
        "    model_name=model_name, description=description, tags=tags\r\n",
        ")\r\n",
        "\r\n",
        "print(\r\n",
        "    remote_run.model_id\r\n",
        ")  # This will be written to the script file later in the notebook."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675237219383
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deploy the model as a Web Service on Azure Container Instance"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.model import InferenceConfig\r\n",
        "from azureml.core.webservice import AciWebservice\r\n",
        "from azureml.core.webservice import Webservice\r\n",
        "from azureml.core.model import Model\r\n",
        "from azureml.core.environment import Environment\r\n",
        "\r\n",
        "inference_config = InferenceConfig(entry_script=script_file_name)\r\n",
        "\r\n",
        "aciconfig = AciWebservice.deploy_configuration(\r\n",
        "    cpu_cores=2,\r\n",
        "    memory_gb=2,\r\n",
        "    tags={\"area\": \"bmData\", \"type\": \"automl_classification\"},\r\n",
        "    description=\"sample service for Automl Classification\",\r\n",
        ")\r\n",
        "\r\n",
        "aci_service_name = model_name.lower()\r\n",
        "print(aci_service_name)\r\n",
        "aci_service = Model.deploy(ws, aci_service_name, [model], inference_config, aciconfig)\r\n",
        "aci_service.wait_for_deployment(True)\r\n",
        "print(aci_service.state)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675237451447
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Call the ACI web service to do the prediction"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\r\n",
        "from numpy import array\r\n",
        "pd.set_option('display.max_rows', None)\r\n",
        "\r\n",
        "sampled_test_data = test_data.take_sample(20)\r\n",
        "X_test = sampled_test_data.drop_columns(columns=[label_column_name]).to_pandas_dataframe()\r\n",
        "y_test = sampled_test_data.keep_columns(columns=[label_column_name], validate=True).to_pandas_dataframe()\r\n",
        "\r\n",
        "X_test_json = X_test.to_json(orient=\"records\")\r\n",
        "data = '{\"data\": ' + X_test_json + \"}\"\r\n",
        "headers = {\"Content-Type\": \"application/json\"}\r\n",
        "\r\n",
        "resp = requests.post(aci_service.scoring_uri, data, headers=headers)\r\n",
        "\r\n",
        "y_pred = json.loads(json.loads(resp.text))[\"result\"]\r\n",
        "\r\n",
        "actual = array(y_test)\r\n",
        "actual = actual[:, 0]\r\n",
        "\r\n",
        "compare_results_df = pd.DataFrame({'actual': actual, 'predicted': y_pred})\r\n",
        "print(compare_results_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675237291359
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Call the ACI web service to do the prediction"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aci_service.delete()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675237291383
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acknowledgements"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Credit Card fraud Detection dataset is made available under the Open Database License: http://opendatacommons.org/licenses/odbl/1.0/. Any rights in individual contents of the database are licensed under the Database Contents License: http://opendatacommons.org/licenses/dbcl/1.0/ and is available at: https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
        "\n",
        "The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (UniversitÃ© Libre de Bruxelles) on big data mining and fraud detection.\n",
        "More details on current and past projects on related topics are available on https://www.researchgate.net/project/Fraud-detection-5 and the page of the DefeatFraud project\n",
        "\n",
        "Please cite the following works:\n",
        "\n",
        "Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n",
        "\n",
        "Dal Pozzolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca. Learned lessons in credit card fraud detection from a practitioner perspective, Expert systems with applications,41,10,4915-4928,2014, Pergamon\n",
        "\n",
        "Dal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, Cesare; Bontempi, Gianluca. Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE\n",
        "\n",
        "Dal Pozzolo, Andrea Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n",
        "\n",
        "Carcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-AÃ«l; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information fusion,41, 182-194,2018,Elsevier\n",
        "\n",
        "Carcillo, Fabrizio; Le Borgne, Yann-AÃ«l; Caelen, Olivier; Bontempi, Gianluca. Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing\n",
        "\n",
        "Bertrand Lebichot, Yann-AÃ«l Le Borgne, Liyun He, Frederic OblÃ©, Gianluca Bontempi Deep-Learning Domain Adaptation Techniques for Credit Cards Fraud Detection, INNSBDDL 2019: Recent Advances in Big Data and Deep Learning, pp 78-88, 2019\n",
        "\n",
        "Fabrizio Carcillo, Yann-AÃ«l Le Borgne, Olivier Caelen, Frederic OblÃ©, Gianluca Bontempi Combining Unsupervised and Supervised Learning in Credit Card Fraud Detection Information Sciences, 2019"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "index_order": 5,
    "nbconvert_exporter": "python",
    "exclude_from_index": false,
    "pygments_lexer": "ipython3",
    "task": "Classification",
    "deployment": [
      "None"
    ],
    "authors": [
      {
        "name": "ratanase"
      }
    ],
    "name": "python",
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "mimetype": "text/x-python",
    "kernel_info": {
      "name": "python38-azureml"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "compute": [
      "AML Compute"
    ],
    "version": "3.6.7",
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "tags": [
      "remote_run",
      "AutomatedML"
    ],
    "datasets": [
      "Creditcard"
    ],
    "file_extension": ".py",
    "categories": [
      "SDK v1",
      "how-to-use-azureml",
      "automated-machine-learning"
    ],
    "category": "tutorial",
    "framework": [
      "None"
    ],
    "friendly_name": "Classification of credit card fraudulent transactions using Automated ML",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
