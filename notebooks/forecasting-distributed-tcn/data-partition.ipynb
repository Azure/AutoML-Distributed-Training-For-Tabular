{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/automated-machine-learning/forecasting-distributed-tcn/data-partition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation - Distributed TCN\n",
    "_**Perform data partition and split the partitioned data into train, validation and test set**_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Dataset, Environment\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.pipeline.steps import PythonScriptStep, ParallelRunConfig, ParallelRunStep\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.data.output_dataset_config import OutputFileDatasetConfig\n",
    "from azureml.data.datapath import DataPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Set up workspace, datastore, experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "experiment = Experiment(ws, \"pre-distributed-tcn\")\n",
    "\n",
    "output = {}\n",
    "output[\"Subscription ID\"] = ws.subscription_id\n",
    "output[\"Workspace\"] = ws.name\n",
    "output[\"SKU\"] = ws.sku\n",
    "output[\"Resource Group\"] = ws.resource_group\n",
    "output[\"Location\"] = ws.location\n",
    "output[\"Experiment Name\"] = experiment.name\n",
    "output[\"SDK Version\"] = azureml.core.VERSION\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "outputDf = pd.DataFrame(data=output, index=[\"\"])\n",
    "outputDf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is expected to reside in any of the datastores of the workspace. If the data is present in a blob container that is not regisitered as a datastore, please refer [this link](https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.datastore(class)?view=azure-ml-py#azureml-core-datastore-register-azure-blob-container) to register blob container as a datastore in the workspace. Update the following variables accordingly:\n",
    "\n",
    "| Variable | Description |\n",
    "| -- | -- |\n",
    "| **datastore_name** | Name of the datastore having data. |\n",
    "| **input_data_path** | Path to the data in the datastore. |\n",
    "| **time_series_id_column_names** | The column names used to uniquely identify timeseries in data that has multiple rows with the same timestamp. |\n",
    "| **time_column_name** | The name of your time column. |\n",
    "| **input_dataset_name** | Name for your dataset. Used in later steps to calculate paths and names for the final processed data. |\n",
    "| **test_split** | Ratio for test data. It is the fraction of rows per time series that will be extracted from the input data to create test data. |\n",
    "| **valid_split** | Ratio for valid data. It is the fraction of rows per time series that will be extracted from the remaining data after test split to create validation data. |\n",
    "\n",
    "<br/>\n",
    "\n",
    "> **Note**: If test_split is 0 (this is possible when test data is already available), test_split won't be performed and valid_split will be directly performed on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_name = datastore.name\n",
    "input_data_path = \"my-data/*.csv\"\n",
    "\n",
    "test_split = 0.2\n",
    "valid_split = 0.2\n",
    "\n",
    "time_series_id_column_names = []\n",
    "time_column_name = \"\"\n",
    "input_dataset_name = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Build the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to create a [compute target](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets#amlcompute) for your AutoML run. In this tutorial, you create AmlCompute as your training compute resource.\n",
    "\n",
    "\\*\\*Creation of AmlCompute takes approximately 5 minutes.**\n",
    "\n",
    "If the AmlCompute with that name is already in your workspace this code will skip the creation process. As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read this [article](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-quotas) on the default limits and how to request more quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"data-partition-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print(\"Found existing cluster, use it.\")\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"STANDARD_D16S_V3\", max_nodes=5\n",
    "    )\n",
    "    compute_target = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Run Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is making sure that the remote pipeline run has all the dependencies needed by the pipeline steps. Dependencies and the runtime context are set by creating and configuring a ``RunConfiguration`` object.\n",
    "\n",
    "The code below shows two options for handling dependencies. As presented, with ``USE_CURATED_ENV = True``, the configuration is based on a [curated environment](https://docs.microsoft.com/en-us/azure/machine-learning/resource-curated-environments). Curated environments have prebuilt Docker images in the [Microsoft Container Registry](https://hub.docker.com/publishers/microsoftowner). For more information, see [Azure Machine Learning curated environments](https://docs.microsoft.com/en-us/azure/machine-learning/resource-curated-environments).\n",
    "\n",
    "The path taken if you change ``USE_CURATED_ENV`` to False shows the pattern for explicitly setting your dependencies. In that scenario, a new custom Docker image will be created and registered in an Azure Container Registry within your resource group (see [Introduction to private Docker container registries in Azure](https://docs.microsoft.com/en-us/azure/container-registry/container-registry-intro)). Building and registering this image can take quite a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_run_config = RunConfiguration()\n",
    "aml_run_config.target = compute_target\n",
    "\n",
    "USE_CURATED_ENV = True\n",
    "if USE_CURATED_ENV:\n",
    "    curated_environment = Environment.get(\n",
    "        workspace=ws, name=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu\"\n",
    "    )\n",
    "    aml_run_config.environment = curated_environment\n",
    "else:\n",
    "    aml_run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "    # Add some packages relied on by data prep step\n",
    "    aml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
    "        conda_packages=[\"pandas\", \"scikit-learn\"],\n",
    "        pip_packages=[\"azureml-sdk\", \"azureml-dataset-runtime[fuse,pandas]\"],\n",
    "        pin_sdk_version=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Data Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Define the output for data partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_dataset_name = f\"{input_dataset_name}_partitioned_{int(time.time())}\"\n",
    "\n",
    "output_path = OutputFileDatasetConfig(\n",
    "    name=\"partitioned_data\", destination=(datastore, partitioned_dataset_name)\n",
    ").as_mount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 Configure ``PythonScriptStep``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is responsible for partitioning the data using ``partition_by`` method.\n",
    "\n",
    "``PythonScriptStep`` arguments\n",
    "| Property | Description |\n",
    "| -- | -- |\n",
    "| **source_directory** | A folder that contains Python script. |\n",
    "| **script_name** | The name of a Python script relative to **source_directory**. |\n",
    "| **arguments** | Command line arguments for the Python script file. |\n",
    "| **compute_target** | The compute target to use. |\n",
    "| **runconfig** | The ``RunConfiguration`` to use. A ``RunConfiguration`` can be used to specify additional requirements for the run, such as conda dependencies and a docker image. |\n",
    "| **allow_reuse** | Indicates whether the step should reuse previous results when re-run with the same settings. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import get_step_args\n",
    "\n",
    "\n",
    "step_args = get_step_args(\n",
    "    time_series_id_column_names,\n",
    "    output_path,\n",
    "    partitioned_dataset_name,\n",
    "    input_data_path,\n",
    "    datastore_name    \n",
    ")\n",
    "\n",
    "data_partition_step = PythonScriptStep(\n",
    "    source_directory=\"./scripts\",\n",
    "    script_name=\"partition.py\",\n",
    "    arguments=step_args,\n",
    "    compute_target=compute_target,\n",
    "    runconfig=aml_run_config,\n",
    "    allow_reuse=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1 Define the output for data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_location = f\"data-partitioned/{input_dataset_name}/split_partitioned_{int(time.time())}\"\n",
    "\n",
    "output_path_split = OutputFileDatasetConfig(\n",
    "    name=\"prepared_data\", destination=(datastore, output_location)\n",
    ").as_mount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2 Configure ``ParallelRunStep``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is responsible for splitting the data into train, validation and test set. The output from previous step will be the input to this step.\n",
    "\n",
    "``ParallelRunConfig`` arguments\n",
    "| Property | Description |\n",
    "| -- | -- |\n",
    "| **source_directory** | Path to folder that contains the ``entry_script`` and supporting files used to execute on compute target. |\n",
    "| **entry_script** | User script which will be run in parallel on multiple nodes. This is specified as a local file path relative to **source_directory**. |\n",
    "| **mini_batch_size** | For FileDataset input, this field is the number of files a user script can process in one ``run()`` call. |\n",
    "| **error_threshold** | The number of file failures that should be ignored during processing. If the error count goes above this value, then the job will be aborted. |\n",
    "| **output_action** | How the output should be organized. Current supported values are 'append_row' and 'summary_only'. |\n",
    "| **environment** | The environment definition is responsible for defining the required application dependencies, such as conda or pip packages. |\n",
    "| **compute_target** | Compute target to use for ParallelRunStep execution. This parameter may be specified as a compute target object or the name of a compute target in the workspace.\n",
    "| **node_count** | Number of nodes in the compute target used for running the ParallelRunStep. |\n",
    "| **process_count_per_node** | The number of worker processes per node to run the entry script in parallel. |\n",
    "\n",
    "``ParallelRunStep`` arguments\n",
    "| Property | Description |\n",
    "| -- | -- |\n",
    "| **name** | Name of the step. Must be unique to the workspace. |\n",
    "| **parallel_run_config** | A ``ParallelRunConfig`` object used to determine required run properties. |\n",
    "| **inputs** | List of input datasets. All datasets in the list should be of same type. |\n",
    "| **arguments** | List of command-line arguments to pass to the Python entry_script. |\n",
    "| **output** | Output port binding, may be used by later pipeline steps. |\n",
    "| **allow_reuse** | Whether the step should reuse previous results when run with the same settings/inputs. |\n",
    "\n",
    "<br/>\n",
    "\n",
    "> **Note**: Please increase the value of argument ``first_task_creation_timeout`` in method ``get_prs_args`` if ``ParallelRunStep`` fails with FirstTaskCreationTimeout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import get_prs_args\n",
    "\n",
    "\n",
    "partitioned_dataset = output_path\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory=\"./scripts\",\n",
    "    entry_script=\"train_valid_test_split.py\",\n",
    "    mini_batch_size=50,\n",
    "    error_threshold=5,\n",
    "    output_action=\"append_row\",\n",
    "    environment=aml_run_config.environment,\n",
    "    compute_target=compute_target,\n",
    "    node_count=5,\n",
    "    process_count_per_node=64,\n",
    ")\n",
    "\n",
    "prs_args = get_prs_args(\n",
    "    time_column_name,\n",
    "    time_series_id_column_names,\n",
    "    test_split,\n",
    "    valid_split\n",
    ")\n",
    "\n",
    "parallel_run_step = ParallelRunStep(\n",
    "    name=\"test valid train split prs\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[partitioned_dataset.as_input(\"partitioned_data\")],\n",
    "    arguments=prs_args,\n",
    "    output=output_path_split,\n",
    "    allow_reuse=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Configure and Submit the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a ``Pipeline`` object that includes data partitioning and data splitting as steps. Then, we submit the pipeline to run. The entire run can take a good amount of time, this majorly depends on the size of the data and compute used. With the config used in this notebook on a 200 GB data, it should roughly take 1.5 to 2 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[data_partition_step, parallel_run_step])\n",
    "run = experiment.submit(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Register the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we register the datasets to the workspace so that they could be used directly in the actual experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import get_partition_str\n",
    "\n",
    "\n",
    "partition_format, partition_path = get_partition_str(time_series_id_column_names)\n",
    "\n",
    "train_ds = Dataset.Tabular.from_parquet_files(\n",
    "    path=(datastore, f\"{output_location}/train/{partition_path}\"),\n",
    "    partition_format=partition_format, validate=False\n",
    ")\n",
    "train_ds.register(ws, f\"{input_dataset_name}_partitioned_train\", create_new_version=True)\n",
    "\n",
    "if valid_split > 0:\n",
    "    valid_ds = Dataset.Tabular.from_parquet_files(\n",
    "        path=(datastore, f\"{output_location}/valid/{partition_path}\"),\n",
    "        partition_format=partition_format, validate=False\n",
    "    )\n",
    "    valid_ds.register(ws, f\"{input_dataset_name}_partitioned_valid\", create_new_version=True)\n",
    "\n",
    "if test_split > 0:\n",
    "    test_ds = Dataset.Tabular.from_parquet_files(\n",
    "        path=(datastore, f\"{output_location}/test/{partition_path}\"),\n",
    "        partition_format=partition_format, validate=False\n",
    "    )\n",
    "    test_ds.register(ws, f\"{input_dataset_name}_partitioned_test\", create_new_version=True)\n",
    "else:\n",
    "    test_ds = Dataset.Tabular.from_delimited_files(path=(datastore, \"<test_data_path>\"), validate=False)\n",
    "    test_ds.register(ws, f\"{input_dataset_name}_test\", create_new_version=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6db9c8d9f0cce2d9127e384e15560d42c3b661994c9f717d0553d1d8985ab1ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
